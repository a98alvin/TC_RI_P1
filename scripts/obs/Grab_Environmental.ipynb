{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58d276ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97f6025a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grab with regular SHIPS\n",
    "\n",
    "# Variables from SHIPS. HSTA and HEND are variables I made up for hours after RI start and hours before RI end.\n",
    "variables = ['VMAX','SHRD','SHTD','VMPI','RSST','RHLO','IR00','DTL','HSTA','HEND']\n",
    "\n",
    "basin = 'north_atlantic' # east_pacific or north_atlantic\n",
    "\n",
    "# Read in RI cases\n",
    "RI = pd.read_csv('/Users/acheung/data/RI_Cases/RI_Events_'+basin+'.csv')\n",
    "\n",
    "# Convert columns to datetime format\n",
    "RI[\"RI Start\"] = pd.to_datetime(RI[\"RI Start\"])\n",
    "RI[\"RI End\"] = pd.to_datetime(RI[\"RI End\"])\n",
    "\n",
    "list_pds = [] # Saves all the information as one big list!\n",
    "\n",
    "for var_ind in range(len(variables)): # Loops through all variables\n",
    "    if (variables[var_ind] == 'HSTA') | (variables[var_ind] == 'HEND'): # For time-hour variables\n",
    "        SHIPS_data = pd.read_csv('/Users/acheung/data/SHIPS/SHIPS_'+variables[0]+'_'+basin+'.csv') # just pick another variable to get time elements\n",
    "    else:\n",
    "        SHIPS_data = pd.read_csv('/Users/acheung/data/SHIPS/SHIPS_'+variables[var_ind]+'_'+basin+'.csv')\n",
    "\n",
    "    SHIPS_data['Time'] = pd.to_datetime(SHIPS_data['Time'])\n",
    "\n",
    "    all_data_arrays = []\n",
    "    all_storm_ids = []\n",
    "    all_storm_times = []\n",
    "    t_start_list = []\n",
    "    t_end_list = []\n",
    "\n",
    "    for i in range(len(RI)):\n",
    "        # Grab current storm data\n",
    "        current_storm_data = SHIPS_data.where(SHIPS_data['Storm_ID'] ==\n",
    "                                                RI.iloc[[i]]['Storm_ID'].values[0]).dropna()\n",
    "        current_storm_data_indexed = current_storm_data.set_index('Time')\n",
    "        current_storm_data_indexed.index = pd.to_datetime(current_storm_data_indexed.index)\n",
    "        # We call shear for 24 hours before RI, but sometimes we dont have data back that far if it was not a TC oficially\n",
    "        \n",
    "        # Grab only times interested from RI (24 h before) to RI end\n",
    "        # Sometimes due to data availability or storm intensity, we may get less than this time period\n",
    "        data_array = current_storm_data_indexed.loc[(RI.iloc[\n",
    "            [i]]['RI Start']+timedelta(hours = -24)).values[0]:RI.iloc[[i]]['RI End'].values[0]]\n",
    "        \n",
    "        # Save storm IDs\n",
    "        all_storm_ids.append(data_array['Storm_ID'].values)\n",
    "        \n",
    "        # Save datetime\n",
    "        all_storm_times.append(data_array.index.values)\n",
    "        \n",
    "        if variables[var_ind] == 'HSTA': # for appending start time variable\n",
    "            time_after_start = data_array.index - RI.iloc[[i]]['RI Start'].values[0]\n",
    "            all_data_arrays.append(time_after_start)\n",
    "            \n",
    "        elif variables[var_ind] == 'HEND': # for appending end time variable\n",
    "            time_before_end = data_array.index - RI.iloc[[i]]['RI End'].values[0]\n",
    "            all_data_arrays.append(time_before_end)\n",
    "        \n",
    "        else: # Append data for when not time-data variables\n",
    "            all_data_arrays.append(data_array[variables[var_ind]].values)\n",
    "\n",
    "    # Creates a pandas dataframe with datetime and atcf id as axis for an individual variable\n",
    "    alldata_pandad = pd.DataFrame(np.concatenate(all_data_arrays),\n",
    "                              index=[np.concatenate(all_storm_ids),\n",
    "                                     np.concatenate(all_storm_times)],columns=[variables[var_ind]])\n",
    "    \n",
    "    # Save all pandas dataframes of each variable with the same axis to a big list!\n",
    "    list_pds.append(alldata_pandad)\n",
    "\n",
    "# Concatenate all the pandas arrays\n",
    "SHIPS_concat = pd.concat(list_pds,axis=1)\n",
    "\n",
    "# Rename the axis\n",
    "SHIPS_concat = SHIPS_concat.rename_axis(('Storm_ID','Time'))\n",
    "\n",
    "# Save to CSV\n",
    "SHIPS_concat.to_csv(\"/Users/acheung/data/RI_Cases/RI_Events_with_SHIPS_Data_\"+basin+\".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0782cc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>VMAX</th>\n",
       "      <th>SHRD</th>\n",
       "      <th>SHTD</th>\n",
       "      <th>VMPI</th>\n",
       "      <th>RSST</th>\n",
       "      <th>RHLO</th>\n",
       "      <th>IR00</th>\n",
       "      <th>DTL</th>\n",
       "      <th>HSTA</th>\n",
       "      <th>HEND</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Storm_ID</th>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">AL032000</th>\n",
       "      <th>2000-08-10 12:00:00</th>\n",
       "      <td>65.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>244.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>28.4</td>\n",
       "      <td>65.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>-1 days +00:00:00</td>\n",
       "      <td>-2 days +00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-08-10 18:00:00</th>\n",
       "      <td>65.0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>294.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>62.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>-1 days +06:00:00</td>\n",
       "      <td>-2 days +06:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-08-11 00:00:00</th>\n",
       "      <td>70.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>262.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>28.6</td>\n",
       "      <td>61.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>397.0</td>\n",
       "      <td>-1 days +12:00:00</td>\n",
       "      <td>-2 days +12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-08-11 06:00:00</th>\n",
       "      <td>75.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>186.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>28.6</td>\n",
       "      <td>60.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>-1 days +18:00:00</td>\n",
       "      <td>-2 days +18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-08-11 12:00:00</th>\n",
       "      <td>80.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>188.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>28.3</td>\n",
       "      <td>59.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>-1 days +00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">AL162022</th>\n",
       "      <th>2022-11-01 18:00:00</th>\n",
       "      <td>45.0</td>\n",
       "      <td>32.1</td>\n",
       "      <td>56.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>-1 days +00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-02 00:00:00</th>\n",
       "      <td>50.0</td>\n",
       "      <td>31.9</td>\n",
       "      <td>67.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>25.9</td>\n",
       "      <td>54.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>0 days 06:00:00</td>\n",
       "      <td>-1 days +06:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-02 06:00:00</th>\n",
       "      <td>55.0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>73.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>59.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>0 days 12:00:00</td>\n",
       "      <td>-1 days +12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-02 12:00:00</th>\n",
       "      <td>65.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>25.5</td>\n",
       "      <td>64.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>0 days 18:00:00</td>\n",
       "      <td>-1 days +18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-02 18:00:00</th>\n",
       "      <td>75.0</td>\n",
       "      <td>31.8</td>\n",
       "      <td>33.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>25.1</td>\n",
       "      <td>67.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>1 days 00:00:00</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              VMAX  SHRD   SHTD   VMPI  RSST  RHLO   IR00  \\\n",
       "Storm_ID Time                                                               \n",
       "AL032000 2000-08-10 12:00:00  65.0   0.9  244.0  143.0  28.4  65.0  999.0   \n",
       "         2000-08-10 18:00:00  65.0   5.6  294.0  142.0  28.5  62.0  999.0   \n",
       "         2000-08-11 00:00:00  70.0   3.1  262.0  141.0  28.6  61.0  999.0   \n",
       "         2000-08-11 06:00:00  75.0   8.8  186.0  142.0  28.6  60.0  999.0   \n",
       "         2000-08-11 12:00:00  80.0   9.4  188.0  137.0  28.3  59.0  999.0   \n",
       "...                            ...   ...    ...    ...   ...   ...    ...   \n",
       "AL162022 2022-11-01 18:00:00  45.0  32.1   56.0  111.0  26.0  52.0   26.0   \n",
       "         2022-11-02 00:00:00  50.0  31.9   67.0  111.0  25.9  54.0   30.0   \n",
       "         2022-11-02 06:00:00  55.0  26.6   73.0  127.0  25.8  59.0   30.0   \n",
       "         2022-11-02 12:00:00  65.0  25.0   48.0  123.0  25.5  64.0   18.0   \n",
       "         2022-11-02 18:00:00  75.0  31.8   33.0  118.0  25.1  67.0   14.0   \n",
       "\n",
       "                                DTL              HSTA              HEND  \n",
       "Storm_ID Time                                                            \n",
       "AL032000 2000-08-10 12:00:00  278.0 -1 days +00:00:00 -2 days +00:00:00  \n",
       "         2000-08-10 18:00:00  332.0 -1 days +06:00:00 -2 days +06:00:00  \n",
       "         2000-08-11 00:00:00  397.0 -1 days +12:00:00 -2 days +12:00:00  \n",
       "         2000-08-11 06:00:00  496.0 -1 days +18:00:00 -2 days +18:00:00  \n",
       "         2000-08-11 12:00:00  398.0   0 days 00:00:00 -1 days +00:00:00  \n",
       "...                             ...               ...               ...  \n",
       "AL162022 2022-11-01 18:00:00  208.0   0 days 00:00:00 -1 days +00:00:00  \n",
       "         2022-11-02 00:00:00  274.0   0 days 06:00:00 -1 days +06:00:00  \n",
       "         2022-11-02 06:00:00  291.0   0 days 12:00:00 -1 days +12:00:00  \n",
       "         2022-11-02 12:00:00  296.0   0 days 18:00:00 -1 days +18:00:00  \n",
       "         2022-11-02 18:00:00  223.0   1 days 00:00:00   0 days 00:00:00  \n",
       "\n",
       "[1390 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SHIPS_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0963fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab with regular EC-SHIPS\n",
    "\n",
    "\n",
    "basin_EC = 'north_atlantic'\n",
    "\n",
    "RI_EC_pre = pd.read_csv('/Users/acheung/data/RI_Cases/RI_Events_'+basin_EC+'.csv')\n",
    "if basin_EC == 'north_atlantic':\n",
    "    EC_SHIPS_shear = pd.read_csv('/Users/acheung/data/EC_SHIPS/EC-SHIPS_deep_layer_shear_atl.csv')\n",
    "elif basin_EC == 'east_pacific':\n",
    "    EC_SHIPS_shear = pd.read_csv('/Users/acheung/data/EC_SHIPS/EC-SHIPS_deep_layer_shear_epac.csv')\n",
    "\n",
    "RI_EC = RI_EC_pre.where(RI_EC_pre['Season'] >= 2016).dropna()\n",
    "    \n",
    "all_EC_shears = []\n",
    "for i in range(len(RI_EC)):\n",
    "    current_storm_shear_EC = EC_SHIPS_shear.where(EC_SHIPS_shear['Storm_ID'] ==\n",
    "                                            RI_EC.iloc[[i]]['Storm_ID'].values[0]).dropna()\n",
    "    if len(current_storm_shear_EC.where(\n",
    "        current_storm_shear_EC['Time'] == RI_EC.iloc[[i]]['RI Start'].values[0]).dropna()['Deep_Layer_Shear']) == 0:\n",
    "        # Some data is missing from EC-SHIPS, so we record NaN in these cases\n",
    "        all_EC_shears.append(np.nan)\n",
    "    else: # When data from EC-SHIPS is not missing\n",
    "        current_shear_RI = current_storm_shear_EC.where(\n",
    "            current_storm_shear_EC['Time'] == RI_EC.iloc[[i]]['RI Start'].values[0]).dropna()['Deep_Layer_Shear'].values[0]\n",
    "        all_EC_shears.append(current_shear_RI)\n",
    "    \n",
    "RI_EC.insert(6, \"Deep-Layer Shear (kt)\", all_EC_shears)\n",
    "RI_EC.to_csv(\"/Users/acheung/data/RI_Cases/RI_Events_with_EC_SHIPS_shear_\"+basin_EC+\".csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885a0878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:besttrack] *",
   "language": "python",
   "name": "conda-env-besttrack-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
