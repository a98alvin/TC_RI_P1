{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58d276ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tropycal.tracks as tracks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e195d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['VMAX','SHRD','SHTD','VMPI','RSST','RHLO','IR00','DTL','HSTA']\n",
    "\n",
    "desired_basin = 'north_atlantic' # east_pacific or north_atlantic\n",
    "year_start = 2000\n",
    "year_end = 2022\n",
    "dt1 = datetime(year=year_start,month=1,day=1)\n",
    "dt2 = datetime(year=year_end,month=12,day=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a5b045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Starting to read in HURDAT2 data\n",
      "--> Completed reading in HURDAT2 data (2.22 seconds)\n"
     ]
    }
   ],
   "source": [
    "# Generate a list of storm names and ATCF IDs\n",
    "\n",
    "basin = tracks.TrackDataset(basin=desired_basin,source='hurdat',include_btk=False)\n",
    "\n",
    "\n",
    "szn_list = []\n",
    "for szn in range(year_start,year_end+1):\n",
    "    one_season = basin.get_season(szn).to_dataframe().set_index(['id'])\n",
    "    szn_list.append(one_season)\n",
    "atcf_id_sname_list = pd.concat(szn_list,axis=0)['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97f6025a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grab with regular SHIPS\n",
    "\n",
    "# Variables from SHIPS. HSTA and HEND are variables I made up for hours after RI start and hours before RI end.\n",
    "variables = ['VMAX','SHRD','SHTD','VMPI','RSST','RHLO','IR00','DTL']\n",
    "\n",
    "basin = 'north_atlantic' # east_pacific or north_atlantic\n",
    "\n",
    "# # Read in RI cases\n",
    "# RI = pd.read_csv('/Users/acheung/data/RI_Cases/RI_Events_'+basin+'.csv')\n",
    "\n",
    "# # Convert columns to datetime format\n",
    "# RI[\"RI Start\"] = pd.to_datetime(RI[\"RI Start\"])\n",
    "# RI[\"RI End\"] = pd.to_datetime(RI[\"RI End\"])\n",
    "\n",
    "list_pds = [] # Saves all the information as one big list!\n",
    "\n",
    "for var_ind in range(len(variables)): # Loops through all variables\n",
    "\n",
    "    SHIPS_data = pd.read_csv('/Users/acheung/data/SHIPS/SHIPS_'+variables[var_ind]+'_'+desired_basin+'.csv')\n",
    "    # Generate storm names list from ATCF IDs\n",
    "    SHIPS_data['Time'] = pd.to_datetime(SHIPS_data['Time'])\n",
    "#     SHIPS_data = SHIPS_data.set_index('Time')\n",
    "    SHIPS_data = SHIPS_data.set_index(['Time','Storm_ID'])\n",
    "    SHIPS_trimmed = SHIPS_data.iloc[(SHIPS_data.index.get_level_values(0) >= dt1 ) &\n",
    "                                    (SHIPS_data.index.get_level_values(0) <= dt2)]\n",
    "    list_pds.append(SHIPS_trimmed)\n",
    "\n",
    "# Concatenate all the pandas arrays\n",
    "SHIPS_concat = pd.concat(list_pds,axis=1)\n",
    "\n",
    "# # Save to CSV\n",
    "# SHIPS_concat.to_csv(\"/Users/acheung/data/RI_Cases/RI_Events_with_SHIPS_Data_\"+basin+\".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0782cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can open any file for generating list of storm names from SHIPS ATCF IDs\n",
    "SHIPS_data = pd.read_csv('/Users/acheung/data/SHIPS/SHIPS_'+variables[0]+'_'+desired_basin+'.csv')\n",
    "\n",
    "# Generate storm names list from ATCF IDs\n",
    "names_list = []\n",
    "for id_ind in range(len(SHIPS_concat.index.get_level_values(1))):\n",
    "    id_now = SHIPS_concat.index.get_level_values(1)[id_ind]\n",
    "    name_now = atcf_id_sname_list.loc[id_now]\n",
    "    names_list.append(name_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f60cf30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_pd = pd.DataFrame(names_list,index=SHIPS_concat.index,columns=['Name'])\n",
    "SHIPS_with_name = pd.concat((SHIPS_concat,names_pd),axis=1)\n",
    "SHIPS_with_name = SHIPS_with_name.reset_index().set_index(['Time','Storm_ID','Name'])\n",
    "# # Save to CSV\n",
    "SHIPS_with_name.to_csv(\"/Users/acheung/data/SHIPS/all_SHIPs_data_combined_\"+desired_basin+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34a28e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>VMAX</th>\n",
       "      <th>SHRD</th>\n",
       "      <th>SHTD</th>\n",
       "      <th>VMPI</th>\n",
       "      <th>RSST</th>\n",
       "      <th>RHLO</th>\n",
       "      <th>IR00</th>\n",
       "      <th>DTL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th>Storm_ID</th>\n",
       "      <th>Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-06-07 18:00:00</th>\n",
       "      <th>AL012000</th>\n",
       "      <th>UNNAMED</th>\n",
       "      <td>25</td>\n",
       "      <td>20.8</td>\n",
       "      <td>96</td>\n",
       "      <td>132</td>\n",
       "      <td>28.3</td>\n",
       "      <td>79</td>\n",
       "      <td>999</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-06-08 00:00:00</th>\n",
       "      <th>AL012000</th>\n",
       "      <th>UNNAMED</th>\n",
       "      <td>25</td>\n",
       "      <td>22.3</td>\n",
       "      <td>93</td>\n",
       "      <td>131</td>\n",
       "      <td>28.2</td>\n",
       "      <td>82</td>\n",
       "      <td>22</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-06-08 06:00:00</th>\n",
       "      <th>AL012000</th>\n",
       "      <th>UNNAMED</th>\n",
       "      <td>25</td>\n",
       "      <td>24.8</td>\n",
       "      <td>98</td>\n",
       "      <td>137</td>\n",
       "      <td>28.3</td>\n",
       "      <td>82</td>\n",
       "      <td>999</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-06-08 12:00:00</th>\n",
       "      <th>AL012000</th>\n",
       "      <th>UNNAMED</th>\n",
       "      <td>25</td>\n",
       "      <td>24.6</td>\n",
       "      <td>111</td>\n",
       "      <td>145</td>\n",
       "      <td>28.4</td>\n",
       "      <td>81</td>\n",
       "      <td>999</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-06-23 00:00:00</th>\n",
       "      <th>AL022000</th>\n",
       "      <th>UNNAMED</th>\n",
       "      <td>25</td>\n",
       "      <td>28.5</td>\n",
       "      <td>267</td>\n",
       "      <td>149</td>\n",
       "      <td>28.4</td>\n",
       "      <td>74</td>\n",
       "      <td>999</td>\n",
       "      <td>466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-10 12:00:00</th>\n",
       "      <th>AL172022</th>\n",
       "      <th>NICOLE</th>\n",
       "      <td>55</td>\n",
       "      <td>26.6</td>\n",
       "      <td>61</td>\n",
       "      <td>92</td>\n",
       "      <td>26.3</td>\n",
       "      <td>70</td>\n",
       "      <td>10</td>\n",
       "      <td>-78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-10 18:00:00</th>\n",
       "      <th>AL172022</th>\n",
       "      <th>NICOLE</th>\n",
       "      <td>40</td>\n",
       "      <td>25.2</td>\n",
       "      <td>42</td>\n",
       "      <td>76</td>\n",
       "      <td>25.3</td>\n",
       "      <td>72</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-11 00:00:00</th>\n",
       "      <th>AL172022</th>\n",
       "      <th>NICOLE</th>\n",
       "      <td>35</td>\n",
       "      <td>25.4</td>\n",
       "      <td>46</td>\n",
       "      <td>61</td>\n",
       "      <td>24.2</td>\n",
       "      <td>69</td>\n",
       "      <td>30</td>\n",
       "      <td>-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-11 06:00:00</th>\n",
       "      <th>AL172022</th>\n",
       "      <th>NICOLE</th>\n",
       "      <td>30</td>\n",
       "      <td>21.1</td>\n",
       "      <td>57</td>\n",
       "      <td>48</td>\n",
       "      <td>23.2</td>\n",
       "      <td>68</td>\n",
       "      <td>18</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-11 12:00:00</th>\n",
       "      <th>AL172022</th>\n",
       "      <th>NICOLE</th>\n",
       "      <td>25</td>\n",
       "      <td>26.3</td>\n",
       "      <td>37</td>\n",
       "      <td>15</td>\n",
       "      <td>21.0</td>\n",
       "      <td>71</td>\n",
       "      <td>18</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8627 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      VMAX  SHRD  SHTD  VMPI  RSST  RHLO  \\\n",
       "Time                Storm_ID Name                                          \n",
       "2000-06-07 18:00:00 AL012000 UNNAMED    25  20.8    96   132  28.3    79   \n",
       "2000-06-08 00:00:00 AL012000 UNNAMED    25  22.3    93   131  28.2    82   \n",
       "2000-06-08 06:00:00 AL012000 UNNAMED    25  24.8    98   137  28.3    82   \n",
       "2000-06-08 12:00:00 AL012000 UNNAMED    25  24.6   111   145  28.4    81   \n",
       "2000-06-23 00:00:00 AL022000 UNNAMED    25  28.5   267   149  28.4    74   \n",
       "...                                    ...   ...   ...   ...   ...   ...   \n",
       "2022-11-10 12:00:00 AL172022 NICOLE     55  26.6    61    92  26.3    70   \n",
       "2022-11-10 18:00:00 AL172022 NICOLE     40  25.2    42    76  25.3    72   \n",
       "2022-11-11 00:00:00 AL172022 NICOLE     35  25.4    46    61  24.2    69   \n",
       "2022-11-11 06:00:00 AL172022 NICOLE     30  21.1    57    48  23.2    68   \n",
       "2022-11-11 12:00:00 AL172022 NICOLE     25  26.3    37    15  21.0    71   \n",
       "\n",
       "                                      IR00  DTL  \n",
       "Time                Storm_ID Name                \n",
       "2000-06-07 18:00:00 AL012000 UNNAMED   999  266  \n",
       "2000-06-08 00:00:00 AL012000 UNNAMED    22  245  \n",
       "2000-06-08 06:00:00 AL012000 UNNAMED   999  235  \n",
       "2000-06-08 12:00:00 AL012000 UNNAMED   999  257  \n",
       "2000-06-23 00:00:00 AL022000 UNNAMED   999  466  \n",
       "...                                    ...  ...  \n",
       "2022-11-10 12:00:00 AL172022 NICOLE     10  -78  \n",
       "2022-11-10 18:00:00 AL172022 NICOLE     30    5  \n",
       "2022-11-11 00:00:00 AL172022 NICOLE     30   -7  \n",
       "2022-11-11 06:00:00 AL172022 NICOLE     18  127  \n",
       "2022-11-11 12:00:00 AL172022 NICOLE     18  346  \n",
       "\n",
       "[8627 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SHIPS_with_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0963fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grab with regular EC-SHIPS\n",
    "\n",
    "\n",
    "# basin_EC = 'north_atlantic'\n",
    "\n",
    "# RI_EC_pre = pd.read_csv('/Users/acheung/data/RI_Cases/RI_Events_'+basin_EC+'.csv')\n",
    "# if basin_EC == 'north_atlantic':\n",
    "#     EC_SHIPS_shear = pd.read_csv('/Users/acheung/data/EC_SHIPS/EC-SHIPS_deep_layer_shear_atl.csv')\n",
    "# elif basin_EC == 'east_pacific':\n",
    "#     EC_SHIPS_shear = pd.read_csv('/Users/acheung/data/EC_SHIPS/EC-SHIPS_deep_layer_shear_epac.csv')\n",
    "\n",
    "# RI_EC = RI_EC_pre.where(RI_EC_pre['Season'] >= 2016).dropna()\n",
    "    \n",
    "# all_EC_shears = []\n",
    "# for i in range(len(RI_EC)):\n",
    "#     current_storm_shear_EC = EC_SHIPS_shear.where(EC_SHIPS_shear['Storm_ID'] ==\n",
    "#                                             RI_EC.iloc[[i]]['Storm_ID'].values[0]).dropna()\n",
    "#     if len(current_storm_shear_EC.where(\n",
    "#         current_storm_shear_EC['Time'] == RI_EC.iloc[[i]]['RI Start'].values[0]).dropna()['Deep_Layer_Shear']) == 0:\n",
    "#         # Some data is missing from EC-SHIPS, so we record NaN in these cases\n",
    "#         all_EC_shears.append(np.nan)\n",
    "#     else: # When data from EC-SHIPS is not missing\n",
    "#         current_shear_RI = current_storm_shear_EC.where(\n",
    "#             current_storm_shear_EC['Time'] == RI_EC.iloc[[i]]['RI Start'].values[0]).dropna()['Deep_Layer_Shear'].values[0]\n",
    "#         all_EC_shears.append(current_shear_RI)\n",
    "    \n",
    "# RI_EC.insert(6, \"Deep-Layer Shear (kt)\", all_EC_shears)\n",
    "# RI_EC.to_csv(\"/Users/acheung/data/RI_Cases/RI_Events_with_EC_SHIPS_shear_\"+basin_EC+\".csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f98b05f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:besttrack] *",
   "language": "python",
   "name": "conda-env-besttrack-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
