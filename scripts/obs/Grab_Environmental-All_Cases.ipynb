{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58d276ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tropycal.tracks as tracks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e195d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "desired_basin = 'north_atlantic' # east_pacific or north_atlantic\n",
    "year_start = 1998\n",
    "year_end = 2024\n",
    "dt1 = datetime(year=year_start,month=1,day=1)\n",
    "dt2 = datetime(year=year_end,month=12,day=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a5b045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Starting to read in HURDAT2 data\n",
      "--> Completed reading in HURDAT2 data (3.24 seconds)\n"
     ]
    }
   ],
   "source": [
    "# Generate a list of storm names and ATCF IDs\n",
    "\n",
    "basin = tracks.TrackDataset(basin=desired_basin,source='hurdat',include_btk=False)\n",
    "\n",
    "\n",
    "szn_list = []\n",
    "for szn in range(year_start,year_end+1):\n",
    "    one_season = basin.get_season(szn).to_dataframe().set_index(['id'])\n",
    "    szn_list.append(one_season)\n",
    "atcf_id_sname_list = pd.concat(szn_list,axis=0)['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97f6025a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grab with regular SHIPS\n",
    "\n",
    "# Variables from SHIPS. HSTA and HEND are variables I made up for hours after RI start and hours before RI end.\n",
    "variables = ['LAT','LON','TYPE','VMAX','SHRD','SHTD','VMPI','COHC','RSST','RHLO','RHMD','IR00','DTL']\n",
    "\n",
    "basin = 'north_atlantic' # east_pacific or north_atlantic\n",
    "\n",
    "# # Read in RI cases\n",
    "# RI = pd.read_csv('/Users/acheung/data/RI_Cases/RI_Events_'+basin+'.csv')\n",
    "\n",
    "# # Convert columns to datetime format\n",
    "# RI[\"RI Start\"] = pd.to_datetime(RI[\"RI Start\"])\n",
    "# RI[\"RI End\"] = pd.to_datetime(RI[\"RI End\"])\n",
    "\n",
    "list_pds = [] # Saves all the information as one big list!\n",
    "\n",
    "for var_ind in range(len(variables)): # Loops through all variables\n",
    "\n",
    "    SHIPS_data = pd.read_csv('/Users/acheung/data/SHIPS/SHIPS_'+variables[var_ind]+'_'+desired_basin+'.csv')\n",
    "    # Generate storm names list from ATCF IDs\n",
    "    SHIPS_data['Time'] = pd.to_datetime(SHIPS_data['Time'])\n",
    "#     SHIPS_data = SHIPS_data.set_index('Time')\n",
    "    SHIPS_data = SHIPS_data.set_index(['Time','Storm_ID'])\n",
    "    SHIPS_trimmed = SHIPS_data.iloc[(SHIPS_data.index.get_level_values(0) >= dt1 ) &\n",
    "                                    (SHIPS_data.index.get_level_values(0) <= dt2)]\n",
    "    list_pds.append(SHIPS_trimmed)\n",
    "\n",
    "# Concatenate all the pandas arrays\n",
    "SHIPS_concat = pd.concat(list_pds,axis=1)\n",
    "\n",
    "# # Save to CSV\n",
    "# SHIPS_concat.to_csv(\"/Users/acheung/data/RI_Cases/RI_Events_with_SHIPS_Data_\"+basin+\".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0782cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can open any file for generating list of storm names from SHIPS ATCF IDs\n",
    "SHIPS_data = pd.read_csv('/Users/acheung/data/SHIPS/SHIPS_'+variables[0]+'_'+desired_basin+'.csv')\n",
    "\n",
    "# Generate storm names list from ATCF IDs\n",
    "names_list = []\n",
    "for id_ind in range(len(SHIPS_concat.index.get_level_values(1))):\n",
    "    id_now = SHIPS_concat.index.get_level_values(1)[id_ind]\n",
    "    name_now = atcf_id_sname_list.loc[id_now]\n",
    "    names_list.append(name_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f60cf30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_pd = pd.DataFrame(names_list,index=SHIPS_concat.index,columns=['Name'])\n",
    "SHIPS_with_name = pd.concat((SHIPS_concat,names_pd),axis=1)\n",
    "SHIPS_with_name = SHIPS_with_name.reset_index().set_index(['Time','Storm_ID','Name'])\n",
    "# # Save to CSV\n",
    "SHIPS_with_name.to_csv(\"/Users/acheung/data/SHIPS/all_SHIPs_data_combined_\"+desired_basin+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a260a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>TYPE</th>\n",
       "      <th>VMAX</th>\n",
       "      <th>SHRD</th>\n",
       "      <th>SHTD</th>\n",
       "      <th>VMPI</th>\n",
       "      <th>COHC</th>\n",
       "      <th>RSST</th>\n",
       "      <th>RHLO</th>\n",
       "      <th>RHMD</th>\n",
       "      <th>IR00</th>\n",
       "      <th>DTL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th>Storm_ID</th>\n",
       "      <th>Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1998-07-27 12:00:00</th>\n",
       "      <th>AL011998</th>\n",
       "      <th>ALEX</th>\n",
       "      <td>11.3</td>\n",
       "      <td>25.4</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1.7</td>\n",
       "      <td>320</td>\n",
       "      <td>126</td>\n",
       "      <td>13</td>\n",
       "      <td>27.4</td>\n",
       "      <td>68</td>\n",
       "      <td>67</td>\n",
       "      <td>26</td>\n",
       "      <td>934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-07-27 18:00:00</th>\n",
       "      <th>AL011998</th>\n",
       "      <th>ALEX</th>\n",
       "      <td>11.7</td>\n",
       "      <td>27.2</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>31</td>\n",
       "      <td>127</td>\n",
       "      <td>14</td>\n",
       "      <td>27.4</td>\n",
       "      <td>69</td>\n",
       "      <td>67</td>\n",
       "      <td>26</td>\n",
       "      <td>1099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-07-28 00:00:00</th>\n",
       "      <th>AL011998</th>\n",
       "      <th>ALEX</th>\n",
       "      <td>12.2</td>\n",
       "      <td>29.2</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5</td>\n",
       "      <td>130</td>\n",
       "      <td>16</td>\n",
       "      <td>27.4</td>\n",
       "      <td>71</td>\n",
       "      <td>68</td>\n",
       "      <td>22</td>\n",
       "      <td>1292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-07-28 06:00:00</th>\n",
       "      <th>AL011998</th>\n",
       "      <th>ALEX</th>\n",
       "      <td>12.6</td>\n",
       "      <td>31.3</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>5.6</td>\n",
       "      <td>34</td>\n",
       "      <td>125</td>\n",
       "      <td>17</td>\n",
       "      <td>27.2</td>\n",
       "      <td>71</td>\n",
       "      <td>69</td>\n",
       "      <td>30</td>\n",
       "      <td>1506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-07-28 12:00:00</th>\n",
       "      <th>AL011998</th>\n",
       "      <th>ALEX</th>\n",
       "      <td>12.9</td>\n",
       "      <td>33.3</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>6.1</td>\n",
       "      <td>66</td>\n",
       "      <td>119</td>\n",
       "      <td>13</td>\n",
       "      <td>27.1</td>\n",
       "      <td>71</td>\n",
       "      <td>70</td>\n",
       "      <td>30</td>\n",
       "      <td>1714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-28 18:00:00</th>\n",
       "      <th>AL202023</th>\n",
       "      <th>TAMMY</th>\n",
       "      <td>33.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>36.6</td>\n",
       "      <td>97</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>999.9</td>\n",
       "      <td>58</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>1312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-23 12:00:00</th>\n",
       "      <th>AL212023</th>\n",
       "      <th>TWENTY-ONE</th>\n",
       "      <td>11.4</td>\n",
       "      <td>82.6</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>3.2</td>\n",
       "      <td>318</td>\n",
       "      <td>169</td>\n",
       "      <td>43</td>\n",
       "      <td>999.9</td>\n",
       "      <td>78</td>\n",
       "      <td>74</td>\n",
       "      <td>14</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-23 18:00:00</th>\n",
       "      <th>AL212023</th>\n",
       "      <th>TWENTY-ONE</th>\n",
       "      <td>11.5</td>\n",
       "      <td>83.2</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>4.3</td>\n",
       "      <td>274</td>\n",
       "      <td>164</td>\n",
       "      <td>40</td>\n",
       "      <td>999.9</td>\n",
       "      <td>79</td>\n",
       "      <td>73</td>\n",
       "      <td>30</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-24 00:00:00</th>\n",
       "      <th>AL212023</th>\n",
       "      <th>TWENTY-ONE</th>\n",
       "      <td>12.2</td>\n",
       "      <td>83.4</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>299</td>\n",
       "      <td>163</td>\n",
       "      <td>41</td>\n",
       "      <td>999.9</td>\n",
       "      <td>79</td>\n",
       "      <td>77</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-24 06:00:00</th>\n",
       "      <th>AL212023</th>\n",
       "      <th>TWENTY-ONE</th>\n",
       "      <td>13.0</td>\n",
       "      <td>83.8</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>3.4</td>\n",
       "      <td>321</td>\n",
       "      <td>166</td>\n",
       "      <td>0</td>\n",
       "      <td>999.9</td>\n",
       "      <td>79</td>\n",
       "      <td>77</td>\n",
       "      <td>10</td>\n",
       "      <td>-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9959 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          LAT   LON  TYPE  VMAX  SHRD  SHTD  \\\n",
       "Time                Storm_ID Name                                             \n",
       "1998-07-27 12:00:00 AL011998 ALEX        11.3  25.4     1    25   1.7   320   \n",
       "1998-07-27 18:00:00 AL011998 ALEX        11.7  27.2     1    25   4.4    31   \n",
       "1998-07-28 00:00:00 AL011998 ALEX        12.2  29.2     1    25   5.9     5   \n",
       "1998-07-28 06:00:00 AL011998 ALEX        12.6  31.3     1    25   5.6    34   \n",
       "1998-07-28 12:00:00 AL011998 ALEX        12.9  33.3     1    30   6.1    66   \n",
       "...                                       ...   ...   ...   ...   ...   ...   \n",
       "2023-10-28 18:00:00 AL202023 TAMMY       33.3  58.0     1    40  36.6    97   \n",
       "2023-10-23 12:00:00 AL212023 TWENTY-ONE  11.4  82.6     1    25   3.2   318   \n",
       "2023-10-23 18:00:00 AL212023 TWENTY-ONE  11.5  83.2     1    25   4.3   274   \n",
       "2023-10-24 00:00:00 AL212023 TWENTY-ONE  12.2  83.4     1    25   4.4   299   \n",
       "2023-10-24 06:00:00 AL212023 TWENTY-ONE  13.0  83.8     1    25   3.4   321   \n",
       "\n",
       "                                         VMPI  COHC   RSST  RHLO  RHMD  IR00  \\\n",
       "Time                Storm_ID Name                                              \n",
       "1998-07-27 12:00:00 AL011998 ALEX         126    13   27.4    68    67    26   \n",
       "1998-07-27 18:00:00 AL011998 ALEX         127    14   27.4    69    67    26   \n",
       "1998-07-28 00:00:00 AL011998 ALEX         130    16   27.4    71    68    22   \n",
       "1998-07-28 06:00:00 AL011998 ALEX         125    17   27.2    71    69    30   \n",
       "1998-07-28 12:00:00 AL011998 ALEX         119    13   27.1    71    70    30   \n",
       "...                                       ...   ...    ...   ...   ...   ...   \n",
       "2023-10-28 18:00:00 AL202023 TAMMY         64     0  999.9    58    51     2   \n",
       "2023-10-23 12:00:00 AL212023 TWENTY-ONE   169    43  999.9    78    74    14   \n",
       "2023-10-23 18:00:00 AL212023 TWENTY-ONE   164    40  999.9    79    73    30   \n",
       "2023-10-24 00:00:00 AL212023 TWENTY-ONE   163    41  999.9    79    77    14   \n",
       "2023-10-24 06:00:00 AL212023 TWENTY-ONE   166     0  999.9    79    77    10   \n",
       "\n",
       "                                          DTL  \n",
       "Time                Storm_ID Name              \n",
       "1998-07-27 12:00:00 AL011998 ALEX         934  \n",
       "1998-07-27 18:00:00 AL011998 ALEX        1099  \n",
       "1998-07-28 00:00:00 AL011998 ALEX        1292  \n",
       "1998-07-28 06:00:00 AL011998 ALEX        1506  \n",
       "1998-07-28 12:00:00 AL011998 ALEX        1714  \n",
       "...                                       ...  \n",
       "2023-10-28 18:00:00 AL202023 TAMMY       1312  \n",
       "2023-10-23 12:00:00 AL212023 TWENTY-ONE   117  \n",
       "2023-10-23 18:00:00 AL212023 TWENTY-ONE    51  \n",
       "2023-10-24 00:00:00 AL212023 TWENTY-ONE    24  \n",
       "2023-10-24 06:00:00 AL212023 TWENTY-ONE   -28  \n",
       "\n",
       "[9959 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SHIPS_with_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0963fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grab with regular EC-SHIPS\n",
    "\n",
    "\n",
    "# basin_EC = 'north_atlantic'\n",
    "\n",
    "# RI_EC_pre = pd.read_csv('/Users/acheung/data/RI_Cases/RI_Events_'+basin_EC+'.csv')\n",
    "# if basin_EC == 'north_atlantic':\n",
    "#     EC_SHIPS_shear = pd.read_csv('/Users/acheung/data/EC_SHIPS/EC-SHIPS_deep_layer_shear_atl.csv')\n",
    "# elif basin_EC == 'east_pacific':\n",
    "#     EC_SHIPS_shear = pd.read_csv('/Users/acheung/data/EC_SHIPS/EC-SHIPS_deep_layer_shear_epac.csv')\n",
    "\n",
    "# RI_EC = RI_EC_pre.where(RI_EC_pre['Season'] >= 2016).dropna()\n",
    "    \n",
    "# all_EC_shears = []\n",
    "# for i in range(len(RI_EC)):\n",
    "#     current_storm_shear_EC = EC_SHIPS_shear.where(EC_SHIPS_shear['Storm_ID'] ==\n",
    "#                                             RI_EC.iloc[[i]]['Storm_ID'].values[0]).dropna()\n",
    "#     if len(current_storm_shear_EC.where(\n",
    "#         current_storm_shear_EC['Time'] == RI_EC.iloc[[i]]['RI Start'].values[0]).dropna()['Deep_Layer_Shear']) == 0:\n",
    "#         # Some data is missing from EC-SHIPS, so we record NaN in these cases\n",
    "#         all_EC_shears.append(np.nan)\n",
    "#     else: # When data from EC-SHIPS is not missing\n",
    "#         current_shear_RI = current_storm_shear_EC.where(\n",
    "#             current_storm_shear_EC['Time'] == RI_EC.iloc[[i]]['RI Start'].values[0]).dropna()['Deep_Layer_Shear'].values[0]\n",
    "#         all_EC_shears.append(current_shear_RI)\n",
    "    \n",
    "# RI_EC.insert(6, \"Deep-Layer Shear (kt)\", all_EC_shears)\n",
    "# RI_EC.to_csv(\"/Users/acheung/data/RI_Cases/RI_Events_with_EC_SHIPS_shear_\"+basin_EC+\".csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f98b05f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fasterenv)",
   "language": "python",
   "name": "fasterenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
