{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5355ae8a-99d2-44c0-b606-48850a43fd3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step one is to identify RI Cases\n",
    "\n",
    "import tropycal.tracks as tracks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb5911a0-1dc8-44bd-8e0e-cc80a3480476",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Starting to read in HURDAT2 data\n",
      "--> Completed reading in HURDAT2 data (1.11 seconds)\n"
     ]
    }
   ],
   "source": [
    "desired_basin = 'east_pacific'\n",
    "\n",
    "basin = tracks.TrackDataset(basin=desired_basin,source='hurdat',include_btk=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5469b32f-ab8d-4330-a5c0-daf0558819fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Years of Evaluation\n",
    "start_year = 2000\n",
    "end_year = 2022\n",
    "ri_threshold = 30 # kt\n",
    "time_difference = 24 # hours (setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8335559c-68ca-4c0b-a0f3-84d27141548e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a dataset of all storm names in year range\n",
    "\n",
    "year_range = range(start_year,end_year+1)\n",
    "\n",
    "all_names_list = []\n",
    "for year_loop in year_range:\n",
    "    curr_year_list = []\n",
    "    sname = basin.get_season(year_loop).to_dataframe()['name'].drop_duplicates(keep=False) # The repeats just drops TDs\n",
    "    ids = basin.get_season(year_loop).to_dataframe()['id']\n",
    "    for loop in sname.index:\n",
    "        if sname[loop] == 'UNNAMED': # skips unnamed storms\n",
    "            pass\n",
    "        else:\n",
    "            if ids[loop][0:2] != 'CP': # don't include central pacific storms\n",
    "                storm_loop = (sname[loop],year_loop)\n",
    "                all_names_list.append(storm_loop)\n",
    "    # all_names_list.append(curr_year_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c93b50c-ba4a-48ef-ba8a-4f354d10c173",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Storm: ('ALETTA', 2000)\n",
      "Current Storm: ('CARLOTTA', 2000)\n",
      "Current Storm: ('DANIEL', 2000)\n",
      "Current Storm: ('ADOLPH', 2001)\n",
      "Current Storm: ('FLOSSIE', 2001)\n",
      "Current Storm: ('JULIETTE', 2001)\n",
      "Current Storm: ('KIKO', 2001)\n",
      "Current Storm: ('NARDA', 2001)\n",
      "Current Storm: ('DOUGLAS', 2002)\n",
      "Current Storm: ('ELIDA', 2002)\n",
      "Current Storm: ('FAUSTO', 2002)\n",
      "Current Storm: ('GENEVIEVE', 2002)\n",
      "Current Storm: ('HERNAN', 2002)\n",
      "Current Storm: ('KENNA', 2002)\n",
      "Current Storm: ('CARLOS', 2003)\n",
      "Current Storm: ('IGNACIO', 2003)\n",
      "Current Storm: ('JIMENA', 2003)\n",
      "Current Storm: ('LINDA', 2003)\n",
      "Current Storm: ('NORA', 2003)\n",
      "Current Storm: ('PATRICIA', 2003)\n",
      "Current Storm: ('DARBY', 2004)\n",
      "Current Storm: ('FRANK', 2004)\n",
      "Current Storm: ('HOWARD', 2004)\n",
      "Current Storm: ('ISIS', 2004)\n",
      "Current Storm: ('JAVIER', 2004)\n",
      "Current Storm: ('FERNANDA', 2005)\n",
      "Current Storm: ('HILARY', 2005)\n",
      "Current Storm: ('KENNETH', 2005)\n",
      "Current Storm: ('MAX', 2005)\n",
      "Current Storm: ('OTIS', 2005)\n",
      "Current Storm: ('BUD', 2006)\n",
      "Current Storm: ('CARLOTTA', 2006)\n",
      "Current Storm: ('DANIEL', 2006)\n",
      "Current Storm: ('HECTOR', 2006)\n",
      "Current Storm: ('ILEANA', 2006)\n",
      "Current Storm: ('JOHN', 2006)\n",
      "Current Storm: ('KRISTY', 2006)\n",
      "Current Storm: ('LANE', 2006)\n",
      "Current Storm: ('PAUL', 2006)\n",
      "Current Storm: ('SERGIO', 2006)\n",
      "Current Storm: ('COSME', 2007)\n",
      "Current Storm: ('FLOSSIE', 2007)\n",
      "Current Storm: ('IVO', 2007)\n",
      "Current Storm: ('HERNAN', 2008)\n",
      "Current Storm: ('NORBERT', 2008)\n",
      "Current Storm: ('CARLOS', 2009)\n",
      "Current Storm: ('FELICIA', 2009)\n",
      "Current Storm: ('GUILLERMO', 2009)\n",
      "Current Storm: ('JIMENA', 2009)\n",
      "Current Storm: ('RICK', 2009)\n",
      "Current Storm: ('CELIA', 2010)\n",
      "Current Storm: ('DARBY', 2010)\n",
      "Current Storm: ('ADRIAN', 2011)\n",
      "Current Storm: ('BEATRIZ', 2011)\n",
      "Current Storm: ('CALVIN', 2011)\n",
      "Current Storm: ('DORA', 2011)\n",
      "Current Storm: ('EUGENE', 2011)\n",
      "Current Storm: ('GREG', 2011)\n",
      "Current Storm: ('HILARY', 2011)\n",
      "Current Storm: ('JOVA', 2011)\n",
      "Current Storm: ('IRWIN', 2011)\n",
      "Current Storm: ('KENNETH', 2011)\n",
      "Current Storm: ('BUD', 2012)\n",
      "Current Storm: ('CARLOTTA', 2012)\n",
      "Current Storm: ('DANIEL', 2012)\n",
      "Current Storm: ('EMILIA', 2012)\n",
      "Current Storm: ('GILMA', 2012)\n",
      "Current Storm: ('MIRIAM', 2012)\n",
      "Current Storm: ('PAUL', 2012)\n",
      "Current Storm: ('BARBARA', 2013)\n",
      "Current Storm: ('GIL', 2013)\n",
      "Current Storm: ('KIKO', 2013)\n",
      "Current Storm: ('MANUEL', 2013)\n",
      "Current Storm: ('RAYMOND', 2013)\n",
      "Current Storm: ('AMANDA', 2014)\n",
      "Current Storm: ('CRISTINA', 2014)\n",
      "Current Storm: ('GENEVIEVE', 2014)\n",
      "Current Storm: ('HERNAN', 2014)\n",
      "Current Storm: ('ISELLE', 2014)\n",
      "Current Storm: ('MARIE', 2014)\n",
      "Current Storm: ('NORBERT', 2014)\n",
      "Current Storm: ('ODILE', 2014)\n",
      "Current Storm: ('SIMON', 2014)\n",
      "Current Storm: ('VANCE', 2014)\n",
      "Current Storm: ('ANDRES', 2015)\n",
      "Current Storm: ('BLANCA', 2015)\n",
      "Current Storm: ('DOLORES', 2015)\n",
      "Current Storm: ('GUILLERMO', 2015)\n",
      "Current Storm: ('HILDA', 2015)\n",
      "Current Storm: ('IGNACIO', 2015)\n",
      "Current Storm: ('JIMENA', 2015)\n",
      "Current Storm: ('LINDA', 2015)\n",
      "Current Storm: ('OLAF', 2015)\n",
      "Current Storm: ('PATRICIA', 2015)\n",
      "Current Storm: ('SANDRA', 2015)\n",
      "Current Storm: ('BLAS', 2016)\n",
      "Current Storm: ('DARBY', 2016)\n",
      "Current Storm: ('GEORGETTE', 2016)\n",
      "Current Storm: ('LESTER', 2016)\n",
      "Current Storm: ('MADELINE', 2016)\n",
      "Current Storm: ('NEWTON', 2016)\n",
      "Current Storm: ('ORLENE', 2016)\n",
      "Current Storm: ('PAINE', 2016)\n",
      "Current Storm: ('ULIKA', 2016)\n",
      "Current Storm: ('SEYMOUR', 2016)\n",
      "Current Storm: ('OTTO', 2016)\n",
      "Current Storm: ('DORA', 2017)\n",
      "Current Storm: ('EUGENE', 2017)\n",
      "Current Storm: ('FERNANDA', 2017)\n",
      "Current Storm: ('HILARY', 2017)\n",
      "Current Storm: ('KENNETH', 2017)\n",
      "Current Storm: ('OTIS', 2017)\n",
      "Current Storm: ('MAX', 2017)\n",
      "Current Storm: ('ALETTA', 2018)\n",
      "Current Storm: ('BUD', 2018)\n",
      "Current Storm: ('HECTOR', 2018)\n",
      "Current Storm: ('JOHN', 2018)\n",
      "Current Storm: ('LANE', 2018)\n",
      "Current Storm: ('NORMAN', 2018)\n",
      "Current Storm: ('OLIVIA', 2018)\n",
      "Current Storm: ('ROSA', 2018)\n",
      "Current Storm: ('SERGIO', 2018)\n",
      "Current Storm: ('WILLA', 2018)\n",
      "Current Storm: ('BARBARA', 2019)\n",
      "Current Storm: ('ERICK', 2019)\n",
      "Current Storm: ('JULIETTE', 2019)\n",
      "Current Storm: ('KIKO', 2019)\n",
      "Current Storm: ('MARIO', 2019)\n",
      "Current Storm: ('DOUGLAS', 2020)\n",
      "Current Storm: ('ELIDA', 2020)\n",
      "Current Storm: ('GENEVIEVE', 2020)\n",
      "Current Storm: ('MARIE', 2020)\n",
      "Current Storm: ('ENRIQUE', 2021)\n",
      "Current Storm: ('FELICIA', 2021)\n",
      "Current Storm: ('HILDA', 2021)\n",
      "Current Storm: ('LINDA', 2021)\n",
      "Current Storm: ('OLAF', 2021)\n",
      "Current Storm: ('RICK', 2021)\n",
      "Current Storm: ('AGATHA', 2022)\n",
      "Current Storm: ('BLAS', 2022)\n",
      "Current Storm: ('DARBY', 2022)\n",
      "Current Storm: ('ESTELLE', 2022)\n",
      "Current Storm: ('HOWARD', 2022)\n",
      "Current Storm: ('KAY', 2022)\n",
      "Current Storm: ('ORLENE', 2022)\n",
      "Current Storm: ('ROSLYN', 2022)\n"
     ]
    }
   ],
   "source": [
    "# Get individual storm\n",
    "\n",
    "all_RI_names = []\n",
    "all_RI_years = []\n",
    "all_RI_starts = []\n",
    "all_RI_ends = []\n",
    "RI_number_all = []\n",
    "storm_ids_all = []\n",
    "\n",
    "def my_fun(x): # This function is used to subtract in rolling functions\n",
    "#     print(x)\n",
    "    return x.iloc[-1] - x.iloc[0]\n",
    "\n",
    "for all_storms_loop in range(len(all_names_list)): # This loops iterates through all storms\n",
    "# for all_storms_loop in [66]:\n",
    "    storm_inds = np.where((basin.get_storm(all_names_list[all_storms_loop]).type=='TS') | \n",
    "         (basin.get_storm(all_names_list[all_storms_loop]).type=='TD') |\n",
    "        (basin.get_storm(all_names_list[all_storms_loop]).type=='SD')|\n",
    "         (basin.get_storm(all_names_list[all_storms_loop]).type=='SS' )|\n",
    "          (basin.get_storm(all_names_list[all_storms_loop]).type=='HU')|\n",
    "           (basin.get_storm(all_names_list[all_storms_loop]).type=='TY')|\n",
    "            (basin.get_storm(all_names_list[all_storms_loop]).type=='ST')) # This ensures we only take TC files, not invests\n",
    "    \n",
    "    storm_time = basin.get_storm(all_names_list[all_storms_loop])['time'][storm_inds] # calls times of storms\n",
    "    storm_intensity = basin.get_storm(all_names_list[all_storms_loop])['vmax'][storm_inds] # calls intensity of storms\n",
    "    storm_id = basin.get_storm(all_names_list[all_storms_loop])['id']\n",
    "    \n",
    "    storm_pd = pd.DataFrame(storm_intensity,index=storm_time) # Generate a dataframe for intensity\n",
    "\n",
    "    # Find time differences\n",
    "    rolling_diff = storm_pd.rolling(window=timedelta(hours=time_difference+6),min_periods=int((time_difference+6)/6),center=False).apply(my_fun).shift(-(int(time_difference/6)))\n",
    "\n",
    "    # This creates a boolean of whether RI threshold is met\n",
    "    threshold_crossings = np.diff(rolling_diff >= ri_threshold, prepend=False)\n",
    "    changes = pd.DataFrame(threshold_crossings).rolling(window=2).apply(my_fun) # Identifies when thresholds met and not met\n",
    "    up_cross = np.where(changes == 1)[0] # Where false -> true\n",
    "    down_cross = np.where(changes == -1)[0] - 1 # Where true -> false. Minus one gets you last qualifying time\n",
    "    \n",
    "    if threshold_crossings.sum() == 0: # No need to record any RIs if there are no RIs detected\n",
    "        pass\n",
    "    else:\n",
    "        RI_start = storm_time[up_cross] # Identify RI start time\n",
    "        if len(down_cross) == 0: # If there are no recorded false -> true, then use 24 h after RI start\n",
    "            RI_end = RI_start\n",
    "        else:\n",
    "            RI_end = storm_time[down_cross] + timedelta(hours=24) # RI end picks up the last time from this RI\n",
    "            if len(up_cross) - len(down_cross) == 1: # sometimes the down cross misses the last cross, this adds it in\n",
    "                RI_end = np.append(RI_end,RI_start[-1]+timedelta(hours=24))        \n",
    "\n",
    "        for RI_same_storm in range(0,len(RI_start)): # This loops unravels the append by RI, so not by storm.\n",
    "            if (RI_start[RI_same_storm].hour == 0 # Take synoptic times only\n",
    "               ) or  (RI_start[RI_same_storm].hour == 6\n",
    "                    ) or (RI_start[RI_same_storm].hour == 12) or(\n",
    "                RI_start[RI_same_storm].hour == 18):\n",
    "                \n",
    "                all_RI_names.append(all_names_list[all_storms_loop][0])\n",
    "                all_RI_years.append(all_names_list[all_storms_loop][1])\n",
    "                all_RI_starts.append(RI_start[RI_same_storm])\n",
    "                all_RI_ends.append(RI_end[RI_same_storm])\n",
    "                RI_number_all.append(RI_same_storm+1)\n",
    "                storm_ids_all.append(storm_id)\n",
    "            else: \n",
    "                pass\n",
    "        \n",
    "        print('Current Storm: ' + str(all_names_list[all_storms_loop]))\n",
    "        # interval that meets the set threshold/time interval. Plus 24 hours includes the 24 hours after the last threshold met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6faf230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas dataframes of RI starts\n",
    "RI_start_pd = pd.DataFrame(all_RI_starts,index=[all_RI_years,all_RI_names,storm_ids_all,RI_number_all],columns=['RI Start'])\n",
    "RI_start_pd.index.names = ['Season','Storm_Name','Storm_ID','RI_Number'] # These are the axis\n",
    "\n",
    "# Create pandas dataframes of RI ends\n",
    "RI_end_pd = pd.DataFrame(all_RI_ends,index=[all_RI_years,all_RI_names,storm_ids_all,RI_number_all],columns=['RI End'])\n",
    "RI_end_pd.index.names = ['Season','Storm_Name','Storm_ID','RI_Number'] # These are the axis\n",
    "\n",
    "# Combines the two pandas dataframes\n",
    "combined_RI_pd = pd.concat([RI_start_pd,RI_end_pd],axis=1)\n",
    "\n",
    "# Create Xarray\n",
    "RI_XR = combined_RI_pd.to_xarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49d834a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as csv and netCDF. Personally, I think the CSV/Pandas Dataframe looks cleaner.\n",
    "\n",
    "combined_RI_pd.to_csv('/Users/acheung/data/RI_Events_'+desired_basin+'.csv')\n",
    "RI_XR.to_netcdf('/Users/acheung/data/RI_Events_'+desired_basin+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8602536c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>RI Start</th>\n",
       "      <th>RI End</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Season</th>\n",
       "      <th>Storm_Name</th>\n",
       "      <th>Storm_ID</th>\n",
       "      <th>RI_Number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <th>JULIETTE</th>\n",
       "      <th>EP112001</th>\n",
       "      <th>2</th>\n",
       "      <td>2001-09-24 18:00:00</td>\n",
       "      <td>2001-09-25 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <th>KENNETH</th>\n",
       "      <th>EP112005</th>\n",
       "      <th>2</th>\n",
       "      <td>2005-09-16 06:00:00</td>\n",
       "      <td>2005-09-17 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <th>NORBERT</th>\n",
       "      <th>EP152008</th>\n",
       "      <th>2</th>\n",
       "      <td>2008-10-10 06:00:00</td>\n",
       "      <td>2008-10-11 06:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2009</th>\n",
       "      <th>CARLOS</th>\n",
       "      <th>EP042009</th>\n",
       "      <th>2</th>\n",
       "      <td>2009-07-13 12:00:00</td>\n",
       "      <td>2009-07-14 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GUILLERMO</th>\n",
       "      <th>EP102009</th>\n",
       "      <th>2</th>\n",
       "      <td>2009-08-14 00:00:00</td>\n",
       "      <td>2009-08-15 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2015</th>\n",
       "      <th>BLANCA</th>\n",
       "      <th>EP022015</th>\n",
       "      <th>2</th>\n",
       "      <td>2015-06-05 12:00:00</td>\n",
       "      <td>2015-06-06 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IGNACIO</th>\n",
       "      <th>EP122015</th>\n",
       "      <th>2</th>\n",
       "      <td>2015-08-28 18:00:00</td>\n",
       "      <td>2015-08-30 06:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <th>LESTER</th>\n",
       "      <th>EP132016</th>\n",
       "      <th>2</th>\n",
       "      <td>2016-08-28 12:00:00</td>\n",
       "      <td>2016-08-30 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2018</th>\n",
       "      <th>NORMAN</th>\n",
       "      <th>EP162018</th>\n",
       "      <th>2</th>\n",
       "      <td>2018-09-01 18:00:00</td>\n",
       "      <td>2018-09-02 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLIVIA</th>\n",
       "      <th>EP172018</th>\n",
       "      <th>2</th>\n",
       "      <td>2018-09-06 00:00:00</td>\n",
       "      <td>2018-09-07 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROSA</th>\n",
       "      <th>EP202018</th>\n",
       "      <th>2</th>\n",
       "      <td>2018-09-26 18:00:00</td>\n",
       "      <td>2018-09-28 12:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                RI Start              RI End\n",
       "Season Storm_Name Storm_ID RI_Number                                        \n",
       "2001   JULIETTE   EP112001 2         2001-09-24 18:00:00 2001-09-25 18:00:00\n",
       "2005   KENNETH    EP112005 2         2005-09-16 06:00:00 2005-09-17 18:00:00\n",
       "2008   NORBERT    EP152008 2         2008-10-10 06:00:00 2008-10-11 06:00:00\n",
       "2009   CARLOS     EP042009 2         2009-07-13 12:00:00 2009-07-14 12:00:00\n",
       "       GUILLERMO  EP102009 2         2009-08-14 00:00:00 2009-08-15 12:00:00\n",
       "2015   BLANCA     EP022015 2         2015-06-05 12:00:00 2015-06-06 12:00:00\n",
       "       IGNACIO    EP122015 2         2015-08-28 18:00:00 2015-08-30 06:00:00\n",
       "2016   LESTER     EP132016 2         2016-08-28 12:00:00 2016-08-30 00:00:00\n",
       "2018   NORMAN     EP162018 2         2018-09-01 18:00:00 2018-09-02 18:00:00\n",
       "       OLIVIA     EP172018 2         2018-09-06 00:00:00 2018-09-07 00:00:00\n",
       "       ROSA       EP202018 2         2018-09-26 18:00:00 2018-09-28 12:00:00"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of how to call in multiindex pandas\n",
    "combined_RI_pd.loc[(combined_RI_pd.index.get_level_values('RI_Number') == 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f063123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EP012000',\n",
       " 'EP032000',\n",
       " 'EP062000',\n",
       " 'EP012001',\n",
       " 'EP072001',\n",
       " 'EP112001',\n",
       " 'EP112001',\n",
       " 'EP122001',\n",
       " 'EP162001',\n",
       " 'EP052002',\n",
       " 'EP092002',\n",
       " 'EP142002',\n",
       " 'EP092003',\n",
       " 'EP102003',\n",
       " 'EP122003',\n",
       " 'EP142003',\n",
       " 'EP052004',\n",
       " 'EP112004',\n",
       " 'EP122004',\n",
       " 'EP132004',\n",
       " 'EP112005',\n",
       " 'EP112005',\n",
       " 'EP132005',\n",
       " 'EP152005',\n",
       " 'EP032006',\n",
       " 'EP052006',\n",
       " 'EP092006',\n",
       " 'EP112006',\n",
       " 'EP132006',\n",
       " 'EP172006',\n",
       " 'EP212006',\n",
       " 'EP062007',\n",
       " 'EP092007',\n",
       " 'EP122007',\n",
       " 'EP092008',\n",
       " 'EP152008',\n",
       " 'EP152008',\n",
       " 'EP042009',\n",
       " 'EP042009',\n",
       " 'EP042009',\n",
       " 'EP102009',\n",
       " 'EP102009',\n",
       " 'EP042010',\n",
       " 'EP052010',\n",
       " 'EP022011',\n",
       " 'EP032011',\n",
       " 'EP042011',\n",
       " 'EP052011',\n",
       " 'EP072011',\n",
       " 'EP092011',\n",
       " 'EP102011',\n",
       " 'EP132011',\n",
       " 'EP022012',\n",
       " 'EP032012',\n",
       " 'EP042012',\n",
       " 'EP052012',\n",
       " 'EP072012',\n",
       " 'EP132012',\n",
       " 'EP162012',\n",
       " 'EP022013',\n",
       " 'EP112013',\n",
       " 'EP132013',\n",
       " 'EP172013',\n",
       " 'EP012014',\n",
       " 'EP032014',\n",
       " 'EP072014',\n",
       " 'EP082014',\n",
       " 'EP092014',\n",
       " 'EP142014',\n",
       " 'EP152014',\n",
       " 'EP192014',\n",
       " 'EP212014',\n",
       " 'EP012015',\n",
       " 'EP022015',\n",
       " 'EP022015',\n",
       " 'EP052015',\n",
       " 'EP092015',\n",
       " 'EP102015',\n",
       " 'EP122015',\n",
       " 'EP122015',\n",
       " 'EP132015',\n",
       " 'EP152015',\n",
       " 'EP192015',\n",
       " 'EP202015',\n",
       " 'EP222015',\n",
       " 'EP032016',\n",
       " 'EP052016',\n",
       " 'EP082016',\n",
       " 'EP132016',\n",
       " 'EP132016',\n",
       " 'EP142016',\n",
       " 'EP152016',\n",
       " 'EP162016',\n",
       " 'EP192016',\n",
       " 'EP222016',\n",
       " 'EP042017',\n",
       " 'EP052017',\n",
       " 'EP062017',\n",
       " 'EP092017',\n",
       " 'EP132017',\n",
       " 'EP152017',\n",
       " 'EP022018',\n",
       " 'EP102018',\n",
       " 'EP122018',\n",
       " 'EP142018',\n",
       " 'EP162018',\n",
       " 'EP162018',\n",
       " 'EP162018',\n",
       " 'EP172018',\n",
       " 'EP172018',\n",
       " 'EP202018',\n",
       " 'EP202018',\n",
       " 'EP212018',\n",
       " 'EP242018',\n",
       " 'EP022019',\n",
       " 'EP062019',\n",
       " 'EP112019',\n",
       " 'EP132019',\n",
       " 'EP142019',\n",
       " 'EP082020',\n",
       " 'EP092020',\n",
       " 'EP182020',\n",
       " 'EP052021',\n",
       " 'EP082021',\n",
       " 'EP122021',\n",
       " 'EP152021',\n",
       " 'EP012022',\n",
       " 'EP022022',\n",
       " 'EP052022',\n",
       " 'EP062022',\n",
       " 'EP092022',\n",
       " 'EP122022',\n",
       " 'EP162022',\n",
       " 'EP192022']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storm_ids_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5814151c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:besttrack] *",
   "language": "python",
   "name": "conda-env-besttrack-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
