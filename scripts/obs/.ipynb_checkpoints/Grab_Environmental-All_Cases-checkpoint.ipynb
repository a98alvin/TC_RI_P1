{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58d276ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tropycal.tracks as tracks\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "e195d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "desired_basin = 'west_pacific' # east_pacific or north_atlantic\n",
    "year_start = 1998\n",
    "year_end = 2024\n",
    "dt1 = datetime(year=year_start,month=1,day=1)\n",
    "dt2 = datetime(year=year_end,month=12,day=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "b4a5b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate a list of storm names and ATCF IDs\n",
    "\n",
    "# basin = tracks.TrackDataset(basin=desired_basin,source='hurdat',include_btk=False)\n",
    "\n",
    "\n",
    "# szn_list = []\n",
    "# for szn in range(year_start,year_end+1):\n",
    "#     one_season = basin.get_season(szn).to_dataframe().set_index(['id'])\n",
    "#     szn_list.append(one_season)\n",
    "# atcf_id_sname_list = pd.concat(szn_list,axis=0)['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "97f6025a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grab with regular SHIPS\n",
    "\n",
    "# Variables from SHIPS. HSTA and HEND are variables I made up for hours after RI start and hours before RI end.\n",
    "variables = ['LAT','LON','TYPE','VMAX','SHRD','SHTD','VMPI','COHC','RSST','RHLO','RHMD','IR00','DTL']\n",
    "\n",
    "basin = desired_basin\n",
    "\n",
    "# # Read in RI cases\n",
    "# RI = pd.read_csv('/Users/acheung/data/RI_Cases/RI_Events_'+basin+'.csv')\n",
    "\n",
    "# # Convert columns to datetime format\n",
    "# RI[\"RI Start\"] = pd.to_datetime(RI[\"RI Start\"])\n",
    "# RI[\"RI End\"] = pd.to_datetime(RI[\"RI End\"])\n",
    "\n",
    "list_pds = [] # Saves all the information as one big list!\n",
    "\n",
    "iteration = 0\n",
    "for var_ind in range(len(variables)): # Loops through all variables\n",
    "    if iteration == 0:\n",
    "        SHIPS_data = pd.read_csv('/Users/acheung/data/SHIPS/SHIPS_'+variables[var_ind]+'_'+desired_basin+'.csv')\n",
    "    else:\n",
    "        SHIPS_data = pd.read_csv('/Users/acheung/data/SHIPS/SHIPS_'+variables[var_ind]+'_'+desired_basin+'.csv',usecols=[1,2])\n",
    "    # Generate storm names list from ATCF IDs\n",
    "    SHIPS_data['Time'] = pd.to_datetime(SHIPS_data['Time'])\n",
    "#     SHIPS_data = SHIPS_data.set_index('Time')\n",
    "    SHIPS_data = SHIPS_data.set_index(['Time'])\n",
    "    SHIPS_trimmed = SHIPS_data.iloc[(SHIPS_data.index.get_level_values(0) >= dt1 ) &\n",
    "                                    (SHIPS_data.index.get_level_values(0) <= dt2)]\n",
    "    list_pds.append(SHIPS_trimmed)\n",
    "    iteration = iteration + 1\n",
    "\n",
    "\n",
    "# Concatenate all the pandas arrays\n",
    "SHIPS_concat = pd.concat(list_pds,axis=1)\n",
    "\n",
    "# # Save to CSV\n",
    "# SHIPS_concat.to_csv(\"/Users/acheung/data/RI_Cases/RI_Events_with_SHIPS_Data_\"+basin+\".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "5bfe55e2-9443-4f07-91e3-2fb4b03a58a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ibtracs = xr.open_dataset('/Users/acheung/data/SHIPS/IBTrACS.ALL.v04r01.nc')\n",
    "needed_data = ibtracs['name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "8e8028a1-12b7-4168-b33a-e01cd91dc60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "atcf_ids_all_time = ibtracs['usa_atcf_id'].sel(date_time=0).astype('str')\n",
    "atcf_id_sname_list_notpd = []\n",
    "for current_id in SHIPS_concat['Storm_ID'].unique():\n",
    "    storm_name = needed_data.where(atcf_ids_all_time == current_id).dropna('storm').astype('str').values\n",
    "    if len(storm_name) == 1:\n",
    "        atcf_id_sname_list_notpd.append(storm_name[0])\n",
    "    else: \n",
    "        atcf_id_sname_list_notpd.append(np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "bd5f975e-a611-4585-a2ef-8a6bba3fb454",
   "metadata": {},
   "outputs": [],
   "source": [
    "atcf_id_sname_list = pd.DataFrame(atcf_id_sname_list_notpd,index=SHIPS_concat['Storm_ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "0782cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can open any file for generating list of storm names from SHIPS ATCF IDs\n",
    "SHIPS_data = pd.read_csv('/Users/acheung/data/SHIPS/SHIPS_'+variables[0]+'_'+desired_basin+'.csv')\n",
    "\n",
    "# Generate storm names list from ATCF IDs\n",
    "names_list = []\n",
    "for id_ind in range(len(SHIPS_concat.index)):\n",
    "    id_now = SHIPS_concat['Storm_ID'].iloc[id_ind]\n",
    "    name_now = atcf_id_sname_list.loc[id_now]\n",
    "    names_list.append(name_now[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "f60cf30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_pd = pd.DataFrame(names_list,index=SHIPS_concat.index,columns=['Name'])\n",
    "SHIPS_with_name = pd.concat((SHIPS_concat,names_pd),axis=1)\n",
    "SHIPS_with_name = SHIPS_with_name.reset_index().set_index(['Time','Storm_ID','Name'])\n",
    "# # Save to CSV\n",
    "SHIPS_with_name.to_csv(\"/Users/acheung/data/SHIPS/all_SHIPs_data_combined_\"+desired_basin+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0963fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grab with regular EC-SHIPS\n",
    "\n",
    "\n",
    "# basin_EC = 'north_atlantic'\n",
    "\n",
    "# RI_EC_pre = pd.read_csv('/Users/acheung/data/RI_Cases/RI_Events_'+basin_EC+'.csv')\n",
    "# if basin_EC == 'north_atlantic':\n",
    "#     EC_SHIPS_shear = pd.read_csv('/Users/acheung/data/EC_SHIPS/EC-SHIPS_deep_layer_shear_atl.csv')\n",
    "# elif basin_EC == 'east_pacific':\n",
    "#     EC_SHIPS_shear = pd.read_csv('/Users/acheung/data/EC_SHIPS/EC-SHIPS_deep_layer_shear_epac.csv')\n",
    "\n",
    "# RI_EC = RI_EC_pre.where(RI_EC_pre['Season'] >= 2016).dropna()\n",
    "    \n",
    "# all_EC_shears = []\n",
    "# for i in range(len(RI_EC)):\n",
    "#     current_storm_shear_EC = EC_SHIPS_shear.where(EC_SHIPS_shear['Storm_ID'] ==\n",
    "#                                             RI_EC.iloc[[i]]['Storm_ID'].values[0]).dropna()\n",
    "#     if len(current_storm_shear_EC.where(\n",
    "#         current_storm_shear_EC['Time'] == RI_EC.iloc[[i]]['RI Start'].values[0]).dropna()['Deep_Layer_Shear']) == 0:\n",
    "#         # Some data is missing from EC-SHIPS, so we record NaN in these cases\n",
    "#         all_EC_shears.append(np.nan)\n",
    "#     else: # When data from EC-SHIPS is not missing\n",
    "#         current_shear_RI = current_storm_shear_EC.where(\n",
    "#             current_storm_shear_EC['Time'] == RI_EC.iloc[[i]]['RI Start'].values[0]).dropna()['Deep_Layer_Shear'].values[0]\n",
    "#         all_EC_shears.append(current_shear_RI)\n",
    "    \n",
    "# RI_EC.insert(6, \"Deep-Layer Shear (kt)\", all_EC_shears)\n",
    "# RI_EC.to_csv(\"/Users/acheung/data/RI_Cases/RI_Events_with_EC_SHIPS_shear_\"+basin_EC+\".csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f98b05f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fasterenv)",
   "language": "python",
   "name": "fasterenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
