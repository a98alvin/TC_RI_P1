{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58d276ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tropycal.tracks as tracks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a8e97be",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['VMAX','SHRD','SHTD','VMPI','RSST','RHLO','IR00','DTL','HSTA']\n",
    "\n",
    "desired_basin = 'north_atlantic' # east_pacific or north_atlantic\n",
    "year_start = 2000\n",
    "year_end = 2022\n",
    "dt1 = datetime(year=year_start,month=1,day=1)\n",
    "dt2 = datetime(year=year_end,month=12,day=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a2f599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Starting to read in HURDAT2 data\n",
      "--> Completed reading in HURDAT2 data (1.45 seconds)\n"
     ]
    }
   ],
   "source": [
    "# Generate a list of storm names and ATCF IDs\n",
    "\n",
    "basin = tracks.TrackDataset(basin=desired_basin,source='hurdat',include_btk=False)\n",
    "\n",
    "\n",
    "szn_list = []\n",
    "for szn in range(year_start,year_end+1):\n",
    "    one_season = basin.get_season(szn).to_dataframe().set_index(['id'])\n",
    "    szn_list.append(one_season)\n",
    "atcf_id_sname_list = pd.concat(szn_list,axis=0)['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97f6025a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grab with regular SHIPS\n",
    "\n",
    "# Variables from SHIPS. HSTA and HEND are variables I made up for hours after RI start and hours before RI end.\n",
    "variables = ['VMAX','SHRD','SHTD','VMPI','RSST','RHLO','IR00','DTL']\n",
    "\n",
    "basin = 'north_atlantic' # east_pacific or north_atlantic\n",
    "\n",
    "# # Read in RI cases\n",
    "# RI = pd.read_csv('/Users/acheung/data/RI_Cases/RI_Events_'+basin+'.csv')\n",
    "\n",
    "# # Convert columns to datetime format\n",
    "# RI[\"RI Start\"] = pd.to_datetime(RI[\"RI Start\"])\n",
    "# RI[\"RI End\"] = pd.to_datetime(RI[\"RI End\"])\n",
    "\n",
    "list_pds = [] # Saves all the information as one big list!\n",
    "\n",
    "for var_ind in range(len(variables)): # Loops through all variables\n",
    "\n",
    "    SHIPS_data = pd.read_csv('/Users/acheung/data/SHIPS/SHIPS_'+variables[var_ind]+'_'+desired_basin+'.csv')\n",
    "    # Generate storm names list from ATCF IDs\n",
    "    SHIPS_data['Time'] = pd.to_datetime(SHIPS_data['Time'])\n",
    "#     SHIPS_data = SHIPS_data.set_index('Time')\n",
    "    SHIPS_data = SHIPS_data.set_index(['Time','Storm_ID'])\n",
    "    SHIPS_trimmed = SHIPS_data.iloc[(SHIPS_data.index.get_level_values(0) >= dt1 ) &\n",
    "                                    (SHIPS_data.index.get_level_values(0) <= dt2)]\n",
    "    list_pds.append(SHIPS_trimmed)\n",
    "\n",
    "# Concatenate all the pandas arrays\n",
    "SHIPS_concat = pd.concat(list_pds,axis=1)\n",
    "\n",
    "# # Save to CSV\n",
    "# SHIPS_concat.to_csv(\"/Users/acheung/data/RI_Cases/RI_Events_with_SHIPS_Data_\"+basin+\".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0782cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can open any file for generating list of storm names from SHIPS ATCF IDs\n",
    "SHIPS_data = pd.read_csv('/Users/acheung/data/SHIPS/SHIPS_'+variables[0]+'_'+desired_basin+'.csv')\n",
    "\n",
    "# Generate storm names list from ATCF IDs\n",
    "names_list = []\n",
    "for id_ind in range(len(SHIPS_concat.index.get_level_values(1))):\n",
    "    id_now = SHIPS_concat.index.get_level_values(1)[id_ind]\n",
    "    name_now = atcf_id_sname_list.loc[id_now]\n",
    "    names_list.append(name_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "975e9adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_pd = pd.DataFrame(names_list,index=SHIPS_concat.index,columns=['Name'])\n",
    "SHIPS_with_name = pd.concat((SHIPS_concat,names_pd),axis=1)\n",
    "SHIPS_with_name = SHIPS_with_name.reset_index().set_index(['Time','Storm_ID','Name'])\n",
    "# # Save to CSV\n",
    "SHIPS_with_name.to_csv(\"/Users/acheung/data/SHIPS/all_SHIPs_data_combined_\"+desired_basin+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4f6afc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>VMAX</th>\n",
       "      <th>SHRD</th>\n",
       "      <th>SHTD</th>\n",
       "      <th>VMPI</th>\n",
       "      <th>RSST</th>\n",
       "      <th>RHLO</th>\n",
       "      <th>IR00</th>\n",
       "      <th>DTL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th>Storm_ID</th>\n",
       "      <th>Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-05-22 12:00:00</th>\n",
       "      <th>EP012000</th>\n",
       "      <th>ALETTA</th>\n",
       "      <td>25</td>\n",
       "      <td>11.5</td>\n",
       "      <td>308</td>\n",
       "      <td>167</td>\n",
       "      <td>30.4</td>\n",
       "      <td>78</td>\n",
       "      <td>6</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-05-22 18:00:00</th>\n",
       "      <th>EP012000</th>\n",
       "      <th>ALETTA</th>\n",
       "      <td>30</td>\n",
       "      <td>10.1</td>\n",
       "      <td>332</td>\n",
       "      <td>159</td>\n",
       "      <td>30.0</td>\n",
       "      <td>75</td>\n",
       "      <td>30</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-05-23 00:00:00</th>\n",
       "      <th>EP012000</th>\n",
       "      <th>ALETTA</th>\n",
       "      <td>30</td>\n",
       "      <td>10.9</td>\n",
       "      <td>320</td>\n",
       "      <td>152</td>\n",
       "      <td>29.7</td>\n",
       "      <td>73</td>\n",
       "      <td>30</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-05-23 06:00:00</th>\n",
       "      <th>EP012000</th>\n",
       "      <th>ALETTA</th>\n",
       "      <td>35</td>\n",
       "      <td>9.0</td>\n",
       "      <td>325</td>\n",
       "      <td>156</td>\n",
       "      <td>29.6</td>\n",
       "      <td>76</td>\n",
       "      <td>30</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-05-23 12:00:00</th>\n",
       "      <th>EP012000</th>\n",
       "      <th>ALETTA</th>\n",
       "      <td>40</td>\n",
       "      <td>13.7</td>\n",
       "      <td>306</td>\n",
       "      <td>158</td>\n",
       "      <td>29.5</td>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-13 00:00:00</th>\n",
       "      <th>CP012019</th>\n",
       "      <th>EMA</th>\n",
       "      <td>40</td>\n",
       "      <td>16.4</td>\n",
       "      <td>54</td>\n",
       "      <td>141</td>\n",
       "      <td>28.2</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-13 06:00:00</th>\n",
       "      <th>CP012019</th>\n",
       "      <th>EMA</th>\n",
       "      <td>35</td>\n",
       "      <td>20.3</td>\n",
       "      <td>67</td>\n",
       "      <td>141</td>\n",
       "      <td>28.2</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-13 12:00:00</th>\n",
       "      <th>CP012019</th>\n",
       "      <th>EMA</th>\n",
       "      <td>35</td>\n",
       "      <td>21.7</td>\n",
       "      <td>72</td>\n",
       "      <td>141</td>\n",
       "      <td>28.2</td>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-13 18:00:00</th>\n",
       "      <th>CP012019</th>\n",
       "      <th>EMA</th>\n",
       "      <td>30</td>\n",
       "      <td>21.4</td>\n",
       "      <td>66</td>\n",
       "      <td>142</td>\n",
       "      <td>28.2</td>\n",
       "      <td>65</td>\n",
       "      <td>6</td>\n",
       "      <td>659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-14 00:00:00</th>\n",
       "      <th>CP012019</th>\n",
       "      <th>EMA</th>\n",
       "      <td>30</td>\n",
       "      <td>26.9</td>\n",
       "      <td>61</td>\n",
       "      <td>137</td>\n",
       "      <td>28.0</td>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8789 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     VMAX  SHRD  SHTD  VMPI  RSST  RHLO  IR00  \\\n",
       "Time                Storm_ID Name                                               \n",
       "2000-05-22 12:00:00 EP012000 ALETTA    25  11.5   308   167  30.4    78     6   \n",
       "2000-05-22 18:00:00 EP012000 ALETTA    30  10.1   332   159  30.0    75    30   \n",
       "2000-05-23 00:00:00 EP012000 ALETTA    30  10.9   320   152  29.7    73    30   \n",
       "2000-05-23 06:00:00 EP012000 ALETTA    35   9.0   325   156  29.6    76    30   \n",
       "2000-05-23 12:00:00 EP012000 ALETTA    40  13.7   306   158  29.5    77     2   \n",
       "...                                   ...   ...   ...   ...   ...   ...   ...   \n",
       "2019-10-13 00:00:00 CP012019 EMA       40  16.4    54   141  28.2    63     2   \n",
       "2019-10-13 06:00:00 CP012019 EMA       35  20.3    67   141  28.2    64    10   \n",
       "2019-10-13 12:00:00 CP012019 EMA       35  21.7    72   141  28.2    66     2   \n",
       "2019-10-13 18:00:00 CP012019 EMA       30  21.4    66   142  28.2    65     6   \n",
       "2019-10-14 00:00:00 CP012019 EMA       30  26.9    61   137  28.0    67     2   \n",
       "\n",
       "                                     DTL  \n",
       "Time                Storm_ID Name         \n",
       "2000-05-22 12:00:00 EP012000 ALETTA  336  \n",
       "2000-05-22 18:00:00 EP012000 ALETTA  316  \n",
       "2000-05-23 00:00:00 EP012000 ALETTA  306  \n",
       "2000-05-23 06:00:00 EP012000 ALETTA  318  \n",
       "2000-05-23 12:00:00 EP012000 ALETTA  350  \n",
       "...                                  ...  \n",
       "2019-10-13 00:00:00 CP012019 EMA     401  \n",
       "2019-10-13 06:00:00 CP012019 EMA     491  \n",
       "2019-10-13 12:00:00 CP012019 EMA     584  \n",
       "2019-10-13 18:00:00 CP012019 EMA     659  \n",
       "2019-10-14 00:00:00 CP012019 EMA     763  \n",
       "\n",
       "[8789 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SHIPS_with_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0963fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grab with regular EC-SHIPS\n",
    "\n",
    "\n",
    "# basin_EC = 'north_atlantic'\n",
    "\n",
    "# RI_EC_pre = pd.read_csv('/Users/acheung/data/RI_Cases/RI_Events_'+basin_EC+'.csv')\n",
    "# if basin_EC == 'north_atlantic':\n",
    "#     EC_SHIPS_shear = pd.read_csv('/Users/acheung/data/EC_SHIPS/EC-SHIPS_deep_layer_shear_atl.csv')\n",
    "# elif basin_EC == 'east_pacific':\n",
    "#     EC_SHIPS_shear = pd.read_csv('/Users/acheung/data/EC_SHIPS/EC-SHIPS_deep_layer_shear_epac.csv')\n",
    "\n",
    "# RI_EC = RI_EC_pre.where(RI_EC_pre['Season'] >= 2016).dropna()\n",
    "    \n",
    "# all_EC_shears = []\n",
    "# for i in range(len(RI_EC)):\n",
    "#     current_storm_shear_EC = EC_SHIPS_shear.where(EC_SHIPS_shear['Storm_ID'] ==\n",
    "#                                             RI_EC.iloc[[i]]['Storm_ID'].values[0]).dropna()\n",
    "#     if len(current_storm_shear_EC.where(\n",
    "#         current_storm_shear_EC['Time'] == RI_EC.iloc[[i]]['RI Start'].values[0]).dropna()['Deep_Layer_Shear']) == 0:\n",
    "#         # Some data is missing from EC-SHIPS, so we record NaN in these cases\n",
    "#         all_EC_shears.append(np.nan)\n",
    "#     else: # When data from EC-SHIPS is not missing\n",
    "#         current_shear_RI = current_storm_shear_EC.where(\n",
    "#             current_storm_shear_EC['Time'] == RI_EC.iloc[[i]]['RI Start'].values[0]).dropna()['Deep_Layer_Shear'].values[0]\n",
    "#         all_EC_shears.append(current_shear_RI)\n",
    "    \n",
    "# RI_EC.insert(6, \"Deep-Layer Shear (kt)\", all_EC_shears)\n",
    "# RI_EC.to_csv(\"/Users/acheung/data/RI_Cases/RI_Events_with_EC_SHIPS_shear_\"+basin_EC+\".csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "376aa12d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:besttrack] *",
   "language": "python",
   "name": "conda-env-besttrack-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
